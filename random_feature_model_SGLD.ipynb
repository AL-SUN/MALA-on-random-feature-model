{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55095189 -0.88060408 -1.3119232   0.78306986  0.35476296]\n",
      "[-0.61558755 -0.8257473  -1.29086896  0.81772004  0.29805785]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "#gaussian distribution sampler: normal(mean=0.0, variance_sqrt=1.0, size=None) s = np.random.normal(0,1)\n",
    "#teacher model\n",
    "#randomly generate z=(x,y) with x evenly sampled from (0,10) and y = 1.5 x + 5, a random noise is added. A total of N samples are drawn.\n",
    "#dimension of x: d\n",
    "d = 10\n",
    "#number of training samples\n",
    "N = 5\n",
    "#dimension of hypothesis space\n",
    "p = 10\n",
    "\n",
    "#point-wise activate function f:tanh\n",
    "#variance of random noise added to y\n",
    "sigma = 0.1\n",
    "#samples X(N*d)\n",
    "X = np.random.normal(0,1,(N,d))\n",
    "#random feature matrix F(d*p)\n",
    "F = np.random.normal(0,1,(d,p))\n",
    "#teacher parameter w(p) with lambda = ? until each dim of Y~1e0\n",
    "lambda_ = 0.0001\n",
    "#w_0 = np.random.normal(0,sigma/np.sqrt(lambda_*N),p)\n",
    "w_0 = np.random.normal(0,1,p)\n",
    "#X after the random feature matrix\n",
    "X_rf = np.tanh(X.dot(F)/np.sqrt(d))\n",
    "Y_pure = X_rf.dot(w_0)\n",
    "Y = Y_pure + np.random.normal(0,sigma,N)\n",
    "print(Y_pure)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05094177,  0.00796192,  0.01405325, -0.01924059,  0.05958275,\n",
       "        0.02287152,  0.04894338, -0.06468644,  0.06222537, -0.0038079 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#empirical risk\n",
    "#L_S(w)\n",
    "def L_S(w):\n",
    "    diff = Y-X_rf.dot(w)\n",
    "    Nloss = diff.dot(diff)\n",
    "    return Nloss/N\n",
    "\n",
    "def grad_L_S(w):\n",
    "    return -2/N*X_rf.T.dot(Y-X_rf.dot(w))\n",
    "\n",
    "def stochastic_grad_L_S(w):\n",
    "    i = int(np.random.rand(1)*N)\n",
    "    X_rand = X_rf[i,:]\n",
    "    Y_rand = Y[i]\n",
    "    return -2*X_rand.T.dot(Y_rand-X_rand.dot(w))\n",
    "\n",
    "stochastic_grad_L_S(w_0+0.1)#when N is small, diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minus log distribution: f\n",
    "#prar beta(also change h if change this)\n",
    "beta = 1000\n",
    "#para sigma_q, can be set according to N.\n",
    "sigma_q = 1\n",
    "def f(w): \n",
    "    return beta*L_S(w)+(1/2/sigma_q**2)*(w.dot(w))\n",
    "\n",
    "def grad_f(w):\n",
    "    grad = beta*grad_L_S(w)\n",
    "    return grad\n",
    "\n",
    "def stochastic_grad_f(w):\n",
    "    grad = stochastic_grad_L_S(w)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32234051 -0.07370167 -1.26855067 -0.8626389  -0.30106368 -0.05358838\n",
      "  1.35701943 -0.09685632 -0.99048833  0.26186646]\n",
      "[ 1.40216662  0.46510099 -1.06503262  0.39042061  0.30560017  0.52184949\n",
      "  2.23327081 -0.0347021  -1.27962318  0.03654264]\n",
      "2.335965990543273\n",
      "3.2345486345966656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.264911419643334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MCMC\n",
    "#stepsize h(h*beta=0.01)\n",
    "h = 0.0001\n",
    "w = np.zeros(p)\n",
    "for i in range(1000000):\n",
    "    grad_f_w = stochastic_grad_f(w)\n",
    "    proposal_state = w-h*grad_f_w+np.sqrt(2*h/beta)*np.random.normal(0,1,p)\n",
    "    w = proposal_state\n",
    "#print(w)\n",
    "#print(w_0)\n",
    "diviant = np.linalg.norm((w-w_0)/np.average(w_0))\n",
    "\n",
    "(w-w_0)/np.average(w_0)\n",
    "\n",
    "print(w[0:20])\n",
    "print(w_0[0:20])\n",
    "print(np.linalg.norm(w))\n",
    "print(np.linalg.norm(w_0))\n",
    "diviant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
